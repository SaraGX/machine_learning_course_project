{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filting cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#case_filter.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "reader=pd.read_csv('/scratch/fs1520/case_tfidf.csv',chunksize=100000)\n",
    "\n",
    "def ran_drop(source,outpath):\n",
    "        a=pd.DataFrame()\n",
    "        counter=0\n",
    "        for i in source:\n",
    "                tp=i.sample(frac=0.5)\n",
    "                if counter==0:\n",
    "                        a=tp\n",
    "                else:\n",
    "                        a=pd.concat([a,tp],ignore_index=True)\n",
    "                counter+=1\n",
    "\n",
    "        #a.to_csv('/scratch/fs1520/case_tfidf.csv')\n",
    "        return(a)\n",
    "\n",
    "rec=ran_drop(reader,'aa.csv')\n",
    "rec.to_csv('/scratch/fs1520/case_tfidf.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge case, nn embedding and zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "ca_op_NN=pd.read_csv('/scratch/fs1520/case_nn.csv')\n",
    "zip_data=pd.read_csv('/scratch/fs1520/zipcode_census.csv')\n",
    "ca_op_NN=ca_op_NN.drop('Unnamed: 0',axis=1).drop('Unnamed: 0_x',axis=1).drop('Unnamed: 0_y',axis=1).drop('district',axis=1)\n",
    "zipdata=zip_data.drop('Unnamed: 0',axis=1)\n",
    "zipdata['EMP']=preprocessing.scale(zipdata['EMP'])\n",
    "zipdata['QP1']=preprocessing.scale(zipdata['QP1'])\n",
    "zipdata['AP']=preprocessing.scale(zipdata['AP'])\n",
    "zipdata['EST']=preprocessing.scale(zipdata['EST'])\n",
    "zipdata['population']=preprocessing.scale(zipdata['population'])\n",
    "zip_data.to_csv('/scratch/fs1520/zip_normed.csv')\n",
    "\n",
    "ca_op_NN=ca_op_NN.fillna(0)\n",
    "ca_op_NN['zip1']=ca_op_NN['zip1'].apply(lambda x: int(x))\n",
    "ca_op_NN_mix=pd.merge(ca_op_NN,zipdata,how='inner',left_on='zip1',right_on='zipcode')\n",
    "ca_op_NN_mix=ca_op_NN_mix.drop('zipcode',axis=1).drop('zip1',axis=1).drop('zip',axis=1)\n",
    "\n",
    "# ca_op_NN_mix['EMP']=preprocessing.scale(ca_op_NN_mix['EMP'])\n",
    "# ca_op_NN_mix['QP1']=preprocessing.scale(ca_op_NN_mix['QP1'])\n",
    "# ca_op_NN_mix['AP']=preprocessing.scale(ca_op_NN_mix['AP'])\n",
    "# ca_op_NN_mix['EST']=preprocessing.scale(ca_op_NN_mix['EST'])\n",
    "# ca_op_NN_mix['population']=preprocessing.scale(ca_op_NN_mix['population'])\n",
    "\n",
    "ca_op_NN_mix.to_csv('/scratch/fs1520/ca_nn_zip.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge case, tfidf embedding and zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "ca_op_tfidf=pd.read_csv('/scratch/fs1520/case_tfidf.csv')\n",
    "zip_data=pd.read_csv('/scratch/fs1520/zipcode_census.csv')\n",
    "ca_op_tfidf=ca_op_tfidf.drop('Unnamed: 0',axis=1).drop('Unnamed: 0.1',axis=1).drop('district',axis=1)\n",
    "zipdata=zip_data.drop('Unnamed: 0',axis=1)\n",
    "zipdata['EMP']=preprocessing.scale(zipdata['EMP'])\n",
    "zipdata['QP1']=preprocessing.scale(zipdata['QP1'])\n",
    "zipdata['AP']=preprocessing.scale(zipdata['AP'])\n",
    "zipdata['EST']=preprocessing.scale(zipdata['EST'])\n",
    "zipdata['population']=preprocessing.scale(zipdata['population'])\n",
    "zip_data.to_csv('/scratch/fs1520/zip_normed.csv')\n",
    "\n",
    "ca_op_tfidf=ca_op_tfidf.fillna(0)\n",
    "ca_op_tfidf['zip1']=ca_op_tfidf['zip1'].apply(lambda x: int(x))\n",
    "ca_op_tfidf_mix=pd.merge(ca_op_tfidf,zipdata,how='inner',left_on='zip1',right_on='zipcode')\n",
    "ca_op_tfidf_mix=ca_op_tfidf_mix.drop('zipcode',axis=1).drop('zip1',axis=1).drop('zip',axis=1)\n",
    "\n",
    "# ca_op_tfidf_mix['EMP']=preprocessing.scale(ca_op_tfidf_mix['EMP'])\n",
    "# ca_op_tfidf_mix['QP1']=preprocessing.scale(ca_op_tfidf_mix['QP1'])\n",
    "# ca_op_tfidf_mix['AP']=preprocessing.scale(ca_op_tfidf_mix['AP'])\n",
    "# ca_op_tfidf_mix['EST']=preprocessing.scale(ca_op_tfidf_mix['EST'])\n",
    "# ca_op_tfidf_mix['population']=preprocessing.scale(ca_op_tfidf_mix['population'])\n",
    "\n",
    "ca_op_tfidf_mix.to_csv('/scratch/fs1520/ca_tfidf_zip.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge case and nn embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def inner_merger(the_cases,the_opinions,lk_1,lk_2,rk_1,rk_2,out_name):\n",
    "    ca_n_op=pd.merge(the_cases,the_opinions,how='inner',left_on=[lk_1,lk_2],right_on=[rk_1,rk_2])\n",
    "    ca_n_op.to_csv(out_name)\n",
    "    return(ca_n_op)\n",
    "\n",
    "cases=pd.read_csv('/scratch/fs1520/case_with_year.csv')\n",
    "cases['judge']=cases['judge'].apply(lambda x:str(x).lower())\n",
    "embedded=pd.read_csv('/scratch/fs1520/embedding.csv')\n",
    "\n",
    "embedded[\"judge\"] = embedded[\"judge_dist\"].map(lambda x:x.split(' ')[0])\n",
    "embedded[\"dist\"] = embedded[\"judge_dist\"].map(lambda x:x.split(' ')[1])\n",
    "embedded=embedded.drop('judge_dist',axis=1)\n",
    "\n",
    "\n",
    "ca_op_NN=inner_merger(cases,embedded,'district','judge','dist','judge','/scratch/fs1520/case_nn.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge case and tfidf embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf merger\n",
    "import pandas as pd\n",
    "def inner_merger(the_cases,the_opinions,lk_1,lk_2,rk_1,rk_2,out_name):\n",
    "    ca_n_op=pd.merge(the_cases,the_opinions,how='inner',left_on=[lk_1,lk_2],right_on=[rk_1,rk_2])\n",
    "    ca_n_op.to_csv(out_name)\n",
    "    return(ca_n_op)\n",
    "\n",
    "cases=pd.read_csv('/scratch/fs1520/case_with_year.csv')\n",
    "cases['judge']=cases['judge'].apply(lambda x:str(x).lower())\n",
    "\n",
    "tfidfed=pd.read_csv('/scratch/fs1520/opinions_tfidf_feature.csv',delimiter=',')\n",
    "tfidfed[\"judge\"] = tfidfed[\"judge_dist\"].map(lambda x:x.split(' ')[0])\n",
    "tfidfed[\"dist\"] = tfidfed[\"judge_dist\"].map(lambda x:x.split(' ')[1])\n",
    "tfidfed=tfidfed.drop('judge_dist',axis=1)\n",
    "\n",
    "ca_op_tfidf=inner_merger(cases,tfidfed,'district','judge','dist','judge','/scratch/fs1520/case_tfidf.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge cases and judge biography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_all=[]\n",
    "reader_all.append(pd.read_stata('/data/Dropbox/Apps/Data/Bankrupty_Courts/Data/Clean/All_data_Part_1.dta',chunksize=10000))\n",
    "reader_all.append(pd.read_stata('/data/Dropbox/Apps/Data/Bankrupty_Courts/Data/Clean/All_data_Part_2.dta',chunksize=10000))\n",
    "reader_all.append(pd.read_stata('/data/Dropbox/Apps/Data/Bankrupty_Courts/Data/Clean/All_data_Part_3.dta',chunksize=10000))\n",
    "\n",
    "\n",
    "#test data\n",
    "for i in reader_all[0]:\n",
    "\tprint(i.columns)\n",
    "\tbreak\n",
    "\n",
    "\n",
    "def judge_case_merger(df1,datasource2,df1_judge,df2_judge,df_inherit=False):\n",
    "#df1: small dataset to be merged datasource2:big one df1_judge:clean_judge df2_judge:judge\n",
    "\tdf_counter=0\n",
    "\tbase_frame=pd.DataFrame()\n",
    "\tfor df2 in datasource2:\n",
    "\t\tdf2['judge']=df2['judge'].apply(lambda x:str(x).lower())\n",
    "\t\ttemp_merged=pd.merge(df2,small_data,how='inner',left_on='judge',right_on='clean_judge')\n",
    "\t\tif df_counter==0:\n",
    "\t\t\tbase_frame=temp_merged\n",
    "\t\telse:\n",
    "\t\t\tbase_frame=pd.concat([base_frame,temp_merged])\n",
    "\t\tdf_counter+=1\n",
    "\treturn(base_frame)\n",
    "\n",
    "merged=[]\n",
    "merged.append(judge_case_merger(small_data,reader_all[0],'judge','clean_judge'))\n",
    "merged.append(judge_case_merger(small_data,reader_all[1],'judge','clean_judge'))\n",
    "merged.append(judge_case_merger(small_data,reader_all[2],'judge','clean_judge'))\n",
    "\n",
    "\n",
    "df2=pd.read_csv('judge_case_unique.csv')\n",
    "df2['0']=df2['0'].apply(lambda x:str(x).lower())\n",
    "df2['clean_judge'].str.split(' ',expand=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering for judges and courts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge_unique(datasource2,df2_judge,df_inherit=False): ###unique judges\n",
    "\tjudge_all=set([])\n",
    "\tfor df2 in datasource2:\n",
    "\t\tjudge_all=judge_all.union(df2[df2_judge].unique())\n",
    "\t\n",
    "\treturn(judge_all)\n",
    "\n",
    "def get_all_unique_judge(source):\n",
    "\tjudge_all_true=set([])\n",
    "\tfor i in source:\n",
    "\t\tjudge_all_true=judge_all_true.union(judge_unique(i,'judge',df_inherit=False))\n",
    "\tpd.DataFrame(list(judge_all_true)).to_csv('/data/WorkData/judge_case_unique.csv')\n",
    "\treturn(judge_all_true)\n",
    "\n",
    "def court_unique(datasource2,df2_court,df2_judge,df_inherit=False): ###unique court\n",
    "\tjudge_all=set([])\n",
    "\tfor df2 in datasource2:\n",
    "\t\tjudge_all=judge_all.union(df2[[df2_court,df2_judge]].apply(tuple,axis=1).unique())\n",
    "\tjudge_pd=pd.DataFrame(judge_all,columns=['district','judge'])\n",
    "\treturn(judge_all)\n",
    "\n",
    "def get_all_unique_court(source):\n",
    "\tcourt_all_true=set([])\n",
    "\tfor i in source:\n",
    "\t\tcourt_all_true=court_all_true.union(court_unique(i,'district','judge',df_inherit=False))\n",
    "\tpd.DataFrame(list(court_all_true)).to_csv('/data/WorkData/court_judge_case_unique.csv')\n",
    "\treturn(court_all_true)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the cases.dta for the whole country, sampling and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_all=[]\n",
    "reader_all.append(pd.read_stata('/data/Dropbox/Apps/Data/Bankrupty_Courts/Data/Clean/All_data_Part_1.dta',chunksize=10000))\n",
    "reader_all.append(pd.read_stata('/data/Dropbox/Apps/Data/Bankrupty_Courts/Data/Clean/All_data_Part_2.dta',chunksize=10000))\n",
    "reader_all.append(pd.read_stata('/data/Dropbox/Apps/Data/Bankrupty_Courts/Data/Clean/All_data_Part_3.dta',chunksize=10000))\n",
    "\n",
    "def fill_disp(datasource,df_inherit=False): ###unique court\n",
    "\tdf_counter=0\n",
    "\tcase_disp=pd.DataFrame()\n",
    "\tfor df in datasource:\n",
    "\t\tdf['disp']=None\n",
    "\t\tdf['the_year']=0\n",
    "\t\tdf.loc[df.loc[:,'dismiss_date']!='','disp']=0\n",
    "\t\tdf.loc[df.loc[:,'discharge_date']!='','disp']=1\n",
    "\t\tdf=df.dropna(subset=['disp'])\n",
    "\t\tdf['close_date']=df['close_date'].apply(lambda x:str(x).split('/')[-1])\n",
    "\t\tdf['close_date']=df['close_date'].apply(lambda x: None if x=='nan' or x=='' else x)\n",
    "\t\tdf['file_date']=df['file_date'].apply(lambda x:str(x).split('/')[-1])\n",
    "\t\tdf['file_date']=df['file_date'].apply(lambda x: None if x=='nan' or x=='' else x)\n",
    "\t\tdf=df.dropna(subset=['close_date'])\n",
    "\t\tdf=df.dropna(subset=['file_date'])\n",
    "\t\tdf['close_date']=df['close_date'].apply(lambda x:int(x)*365+np.random.randint(365))\n",
    "\t\tdf['file_date']=df['file_date'].apply(lambda x:int(x)*365)\n",
    "\t\tdf['time_dif']=df['close_date']-df['file_date']\n",
    "\t\tdf_tp=df[['district','judge','time_dif']] #zip #\n",
    "\t\tif df_counter==0:\n",
    "\t\t\tcase_disp=df_tp\n",
    "\t\telse:\n",
    "\t\t\tcase_disp=pd.concat([case_disp, df_tp], ignore_index=True)\n",
    "\t\tdf_counter+=1\n",
    "\treturn(case_disp)\n",
    "\n",
    "def merge_all_case(source):\n",
    "\tsource_counter=0\n",
    "\tcase_all=pd.DataFrame()\n",
    "\tfor i in source:\n",
    "\t\tif source_counter==0:\n",
    "\t\t\tcase_all=fill_disp(i)\n",
    "\t\telse:\n",
    "\t\t\tcase_all=pd.concat([case_all, fill_disp(i)], ignore_index=True)\n",
    "\t\tsource_counter+=1\n",
    "\tcase_all.to_csv('/data/WorkData/all_cases_merged_disp.csv')\n",
    "\treturn(case_all)\n",
    "\n",
    "all_cases_new=merge_all_case(reader_all)\n",
    "\n",
    "df2=pd.DataFrame({'mean_time':all_cases_new.groupby(['district','judge'])['time_dif'].mean()}).reset_index()\n",
    "df2.to_csv('data/WorkData/cases_timedif.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
